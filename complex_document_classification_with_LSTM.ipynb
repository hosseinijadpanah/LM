{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Complex Document Classification\n",
        "For a more complex document classification task, we can consider using a larger dataset and a neural network model, such as a Convolutional Neural Network (CNN) or a Long Short-Term Memory (LSTM) network. These models are well-suited for text classification tasks, especially when dealing with large datasets.\n",
        "\n",
        "\n",
        "**for simple document classification ---> using TF-IDF & Naive Bayes Classifier**\n",
        "\n",
        "\n",
        "**for complex document classification ---> using Long Short-Term Memory (LSTM)**\n",
        "\n",
        "One such dataset is the \"Jigsaw Multilingual Toxic Comment Classification\" dataset available on Kaggle. It contains comments from various online platforms and is labeled for toxicity. However, for this example, I will use a simpler dataset due to accessibility and ease of use in an example. Let's use the \"IMDb Movie Reviews\" dataset, which is large and commonly used for text classification tasks. It contains movie reviews, labeled as positive or negative.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G-gFTk8bFOLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup and Data Loading\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "# Set parameters for the dataset and model\n",
        "max_features = 20000  # number of words to consider as features\n",
        "maxlen = 80  # cut texts after this number of words\n",
        "batch_size = 32\n",
        "\n",
        "# Load the IMDb dataset\n",
        "print(\"Loading data...\")\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "# Pad sequences for consistency\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDqVpGJ7GBL0",
        "outputId": "55609019-2aae-4036-b63b-3fc6f53fe3cc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 1s 0us/step\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 80)\n",
            "x_test shape: (25000, 80)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the LSTM Model\n",
        "\n",
        "# LSTM model architecture\n",
        "print('Building model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UnwLtXIGCGJ",
        "outputId": "5edf6f8f-2688-43cc-c1ff-bc1a92314b73"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the Model\n",
        "# Train the model\n",
        "print('Train...')\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=15,\n",
        "          validation_data=(x_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvLCsLAwGCSe",
        "outputId": "90346ef5-1e3b-4051-d0a6-aa585de8b484"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train...\n",
            "Epoch 1/15\n",
            "782/782 [==============================] - 257s 321ms/step - loss: 0.4431 - accuracy: 0.7870 - val_loss: 0.3789 - val_accuracy: 0.8278\n",
            "Epoch 2/15\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.2585 - accuracy: 0.8975 - val_loss: 0.3677 - val_accuracy: 0.8360\n",
            "Epoch 3/15\n",
            "782/782 [==============================] - 235s 300ms/step - loss: 0.1636 - accuracy: 0.9376 - val_loss: 0.4450 - val_accuracy: 0.8321\n",
            "Epoch 4/15\n",
            "782/782 [==============================] - 234s 299ms/step - loss: 0.1081 - accuracy: 0.9606 - val_loss: 0.5224 - val_accuracy: 0.8248\n",
            "Epoch 5/15\n",
            "782/782 [==============================] - 235s 300ms/step - loss: 0.0749 - accuracy: 0.9731 - val_loss: 0.6590 - val_accuracy: 0.8208\n",
            "Epoch 6/15\n",
            "782/782 [==============================] - 235s 301ms/step - loss: 0.0524 - accuracy: 0.9826 - val_loss: 0.6129 - val_accuracy: 0.8170\n",
            "Epoch 7/15\n",
            "782/782 [==============================] - 236s 302ms/step - loss: 0.0342 - accuracy: 0.9889 - val_loss: 0.7256 - val_accuracy: 0.8157\n",
            "Epoch 8/15\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.0288 - accuracy: 0.9901 - val_loss: 0.7740 - val_accuracy: 0.8174\n",
            "Epoch 9/15\n",
            "782/782 [==============================] - 238s 305ms/step - loss: 0.0226 - accuracy: 0.9924 - val_loss: 0.8535 - val_accuracy: 0.8144\n",
            "Epoch 10/15\n",
            "782/782 [==============================] - 236s 301ms/step - loss: 0.0191 - accuracy: 0.9939 - val_loss: 0.8811 - val_accuracy: 0.8118\n",
            "Epoch 11/15\n",
            "782/782 [==============================] - 234s 299ms/step - loss: 0.0125 - accuracy: 0.9965 - val_loss: 0.9908 - val_accuracy: 0.8096\n",
            "Epoch 12/15\n",
            "782/782 [==============================] - 234s 299ms/step - loss: 0.0133 - accuracy: 0.9958 - val_loss: 0.8674 - val_accuracy: 0.8058\n",
            "Epoch 13/15\n",
            "782/782 [==============================] - 233s 298ms/step - loss: 0.0084 - accuracy: 0.9974 - val_loss: 1.1824 - val_accuracy: 0.8021\n",
            "Epoch 14/15\n",
            "782/782 [==============================] - 234s 299ms/step - loss: 0.0069 - accuracy: 0.9979 - val_loss: 1.2578 - val_accuracy: 0.8097\n",
            "Epoch 15/15\n",
            "782/782 [==============================] - 234s 299ms/step - loss: 0.0134 - accuracy: 0.9961 - val_loss: 1.0700 - val_accuracy: 0.8133\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7aade21c71f0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the Model\n",
        "# Evaluate the model\n",
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jn5c4BsMGLxN",
        "outputId": "0eca6002-6736-4344-ad1a-21669c609681"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 23s 29ms/step - loss: 1.0700 - accuracy: 0.8133\n",
            "Test score: 1.0700082778930664\n",
            "Test accuracy: 0.813319981098175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some common types of visualizations for model evaluation:\n",
        "\n",
        "- Training and Validation Loss over Epochs:\n",
        "\n",
        "This line chart shows the training and validation loss over each training epoch. It helps you assess whether your model is overfitting or underfitting.\n",
        "\n",
        "- Training and Validation Accuracy over Epochs:\n",
        "\n",
        "Similar to the loss chart, this line chart displays the training and validation accuracy over each epoch. It helps you understand how well your model is performing on the training and validation data.\n",
        "\n",
        "- Confusion Matrix:\n",
        "\n",
        "A confusion matrix is a table that helps you visualize the performance of a classification model. It shows the number of true positives, true negatives, false positives, and false negatives. You can create a heatmap to display this information.\n",
        "\n",
        "- ROC Curve and AUC:\n",
        "\n",
        "If your problem is binary classification, you can create a Receiver Operating Characteristic (ROC) curve and calculate the Area Under the Curve (AUC). This curve helps you evaluate the trade-off between true positive rate and false positive rate at different thresholds.\n",
        "\n",
        "- Precision-Recall Curve:\n",
        "\n",
        "This curve is useful for imbalanced datasets and shows the trade-off between precision and recall at different thresholds. It helps you choose an appropriate threshold for your classification task.\n",
        "\n",
        "- Histogram of Predictions:\n",
        "\n",
        "A histogram can be used to visualize the distribution of predicted probabilities or scores for each class. It can help you understand how confident your model is in its predictions.\n",
        "\n",
        "- Box Plot of Prediction Scores:\n",
        "\n",
        "A box plot can help you visualize the spread of prediction scores for each class, providing insights into the model's uncertainty."
      ],
      "metadata": {
        "id": "bs0bzzOrVuOr"
      }
    }
  ]
}