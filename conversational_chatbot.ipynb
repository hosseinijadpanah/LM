{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Create Chatbot\n",
        "Here's an example of how to create a simple chatbot in Google Colab using the Transformers library with the GPT-3 model.\n",
        "\n",
        "Building a conversational chatbot using a language model can be achieved using the Hugging Face Transformers library, which provides access to pre-trained language models suitable for natural language understanding and generation tasks.\n",
        "\n",
        "Explanation of Important Lines:\n",
        "\n",
        "- Import necessary libraries: We import the required libraries, including the Transformers library.\n",
        "\n",
        "- Load the pre-trained GPT-2 model and tokenizer: We load a pre-trained GPT-2 model and its corresponding tokenizer. You can choose from different model sizes (e.g., \"gpt2-medium,\" \"gpt2-large\") depending on your requirements.\n",
        "\n",
        "- Set the chatbot in \"response generation\" mode: We set the model to evaluation mode (model.eval()) to disable training-related behavior.\n",
        "\n",
        "- Define the chat_with_bot function: This function allows the user to interact with the chatbot in a loop. It encodes the user's input, generates a response, and prints the chatbot's reply.\n",
        "\n",
        "- Encode user input and generate a response: We encode the user's input using the tokenizer, and then use the model to generate a response based on the input. The max_length parameter limits the response length.\n",
        "\n",
        "- Decode and print the chatbot's response: We decode the model's generated output and print it as the chatbot's response.\n",
        "\n",
        "- Start a conversation with the chatbot: We initiate the chat with a greeting from the chatbot and call the chat_with_bot function to allow user interaction.\n"
      ],
      "metadata": {
        "id": "6BCwKGM3fzDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Transformers library\n",
        "!pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtO1lfVPfzU7",
        "outputId": "ce1f8d8b-3482-4bb8-d226-d6a52b39a60c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the pre-trained GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"  # You can use other variants like \"gpt2-medium\", \"gpt2-large\" for more capabilities\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Set the chatbot in \"response generation\" mode\n",
        "model.eval()\n",
        "\n",
        "# Define a function for chatbot interaction\n",
        "def chat_with_bot():\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        # Exit the loop if the user types \"exit\"\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Encode the user's input and generate a response\n",
        "        input_ids = tokenizer.encode(\"You: \" + user_input, return_tensors=\"pt\")\n",
        "        output = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
        "\n",
        "        # Decode and print the chatbot's response\n",
        "        chatbot_response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        print(\"Chatbot:\", chatbot_response)\n",
        "\n",
        "# Start a conversation with the chatbot\n",
        "print(\"Chatbot: Hello! How can I assist you today?\")\n",
        "chat_with_bot()\n",
        "\n",
        "\n",
        "# testing\n",
        "# \"What's your favorite movie?\"\n",
        "# \"Are you a robot or a human?\"\n",
        "# \"What is your gender?\"\n",
        "# \"What is your purpose?\"\n",
        "# \"Tell me a funny story.\"\n",
        "# not \"exit\" ----> its not good format\n",
        "# Exit\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCHyIRQSgtCi",
        "outputId": "3b364e01-ed6b-4729-b189-28cef9fd5c5d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot: Hello! How can I assist you today?\n",
            "You: \"What's your favorite movie?\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot: You: \"What's your favorite movie?\"\n",
            "\n",
            "A: \"The one with the most beautiful woman in the world.\"\n",
            "\n",
            "Q: \"What's your favorite movie?\"\n",
            "\n",
            "A: \"The one with the most beautiful woman in the world.\"\n",
            "\n",
            "Q: \"What's your favorite movie?\"\n",
            "\n",
            "A: \"The one with the most beautiful woman in the world.\"\n",
            "\n",
            "Q: \"What's your favorite movie?\"\n",
            "\n",
            "A: \"The one with the most\n",
            "You: \"Are you a robot or a human?\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot: You: \"Are you a robot or a human?\"\n",
            "\n",
            "A: \"I'm a robot.\"\n",
            "\n",
            "Q: \"What's your favorite movie?\"\n",
            "\n",
            "A: \"The Matrix.\"\n",
            "\n",
            "Q: \"What's your favorite movie?\"\n",
            "\n",
            "A: \"The Matrix.\"\n",
            "\n",
            "Q: \"What's your favorite movie?\"\n",
            "\n",
            "A: \"The Matrix.\"\n",
            "\n",
            "Q: \"What's your favorite movie?\"\n",
            "\n",
            "A: \"The Matrix.\"\n",
            "\n",
            "Q\n",
            "You: \"What is your gender?\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot: You: \"What is your gender?\"\n",
            "\n",
            "A: \"I'm a woman.\"\n",
            "\n",
            "Q: \"What is your gender?\"\n",
            "\n",
            "A: \"I'm a man.\"\n",
            "\n",
            "Q: \"What is your gender?\"\n",
            "\n",
            "A: \"I'm a woman.\"\n",
            "\n",
            "Q: \"What is your gender?\"\n",
            "\n",
            "A: \"I'm a man.\"\n",
            "\n",
            "Q: \"What is your gender?\"\n",
            "\n",
            "A: \"I'm a woman.\"\n",
            "\n",
            "\n",
            "You: \"What is your purpose?\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot: You: \"What is your purpose?\"\n",
            "\n",
            "A: \"I'm going to be a doctor.\"\n",
            "\n",
            "Q: \"What is your purpose?\"\n",
            "\n",
            "A: \"I'm going to be a doctor.\"\n",
            "\n",
            "Q: \"What is your purpose?\"\n",
            "\n",
            "A: \"I'm going to be a doctor.\"\n",
            "\n",
            "Q: \"What is your purpose?\"\n",
            "\n",
            "A: \"I'm going to be a doctor.\"\n",
            "\n",
            "Q: \"What is your purpose?\"\n",
            "You: \"Tell me a funny story.\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: You: \"Tell me a funny story.\"\n",
            "\n",
            "A: \"I'm a little bit of a nerd.\"\n",
            "\n",
            "Q: \"What's your favorite movie?\"\n",
            "\n",
            "A: \"The one about the first time you see a girl in a bikini.\"\n",
            "\n",
            "Q: \"What's your favorite movie?\"\n",
            "\n",
            "A: \"The one about the first time you see a girl in a bikini.\"\n",
            "\n",
            "Q: \"What's your favorite movie?\"\n",
            "\n",
            "A: \"The\n",
            "You: Exit\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # more complex chatbot example\n",
        " more complex chatbot with efficient and clear answers can be developed using the Hugging Face Transformers library. In this example, we'll use the DialoGPT model, which is specifically designed for multi-turn conversations.\n",
        "\n",
        "in this example:\n",
        "\n",
        "We load the **DialoGPT mode**l and tokenizer designed for multi-turn conversations.\n",
        "\n",
        "The conversation_history variable is used to maintain the conversation context.\n",
        "\n",
        "We initiate the chatbot with a greeting and a prompt for the user.\n",
        "\n",
        "Inside the loop, user inputs are processed, and if the user types 'exit', the conversation ends.\n",
        "\n",
        "User inputs are added to the conversation history.\n",
        "\n",
        "The model generates responses based on the entire conversation history, ensuring context awareness.\n",
        "\n",
        "We use no_repeat_ngram_size and top_p parameters to encourage diverse and contextually relevant responses.\n",
        "\n",
        "The chatbot's response is displayed, and it is also added to the conversation history.\n",
        "\n",
        "This chatbot is more advanced, capable of handling multi-turn conversations, maintaining context, and providing coherent and contextually relevant responses. It can be further enhanced by adding custom prompts, fine-tuning, and incorporating external APIs for more specific tasks."
      ],
      "metadata": {
        "id": "vEmYE5SMiSwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the DialoGPT model and tokenizer\n",
        "model_name = \"microsoft/DialoGPT-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Initialize the conversation history\n",
        "conversation_history = []\n",
        "\n",
        "print(\"Chatbot: Hello! How can I assist you today? (Type 'exit' to end the conversation)\")\n",
        "\n",
        "def get_response(conversation_history):\n",
        "    # Join the conversation history and encode it\n",
        "    new_user_input_ids = tokenizer.encode(conversation_history[-1] + tokenizer.eos_token, return_tensors='pt')\n",
        "    bot_input_ids = torch.cat([torch.tensor(tokenizer.encode(\"\\n\".join(conversation_history[-6:-1]))), new_user_input_ids], dim=-1) if len(conversation_history) > 1 else new_user_input_ids\n",
        "\n",
        "    # Generate a response\n",
        "    chatbot_output = model.generate(bot_input_ids, max_length=150, pad_token_id=tokenizer.eos_token_id, no_repeat_ngram_size=2, do_sample=True, top_p=0.9, temperature=0.8)\n",
        "    response = tokenizer.decode(chatbot_output[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "\n",
        "    return response\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Exit the conversation if the user types 'exit'\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Add user input to the conversation history\n",
        "    conversation_history.append(\"You: \" + user_input)\n",
        "\n",
        "    # Generate a response and display it\n",
        "    chatbot_response = get_response(conversation_history)\n",
        "    print(\"Chatbot:\", chatbot_response)\n",
        "\n",
        "    # Add chatbot response to the conversation history\n",
        "    conversation_history.append(\"Chatbot: \" + chatbot_response)\n",
        "\n",
        "    # \"What's your favorite book?\"\n",
        "    # \"I'm feeling stressed about work. Any tips?\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_D5VLrBi1_G",
        "outputId": "76138d38-b865-4ae3-bef3-f95ee28d89d8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot: Hello! How can I assist you today? (Type 'exit' to end the conversation)\n",
            "You: \"I'm feeling stressed about work. Any tips?\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: Don't be stressed. Just don't make it worse.\n",
            "You: Exit\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "if you're finding the responses unsatisfactory, there are several ways you can tweak or enhance your chatbot's performance. Here are some suggestions:\n",
        "\n",
        "- Fine-Tuning on Specific Data: DialoGPT is a general-purpose conversational model. If your application requires expertise in a specific domain, consider fine-tuning the model on a dataset that's relevant to that domain.\n",
        "\n",
        "- Improved Context Management: The model's ability to generate relevant responses depends heavily on the context it's given. You might want to implement better context management, such as truncating the conversation history to the most recent exchanges or summarizing longer histories.\n",
        "\n",
        "- Adjusting Generation Parameters: You can experiment with different parameters in the model.generate function like max_length, num_return_sequences, no_repeat_ngram_size, and top_p to see if they yield better results. For instance, adjusting temperature and top_k can significantly alter the style and quality of responses.\n",
        "\n",
        "- Hybrid Approach: Incorporate rule-based elements for specific types of queries where precision is key. For example, if the chatbot is frequently asked about certain topics, you can have predefined, reliable responses or heuristics to handle these.\n",
        "\n",
        "- Feedback Loop: Implement a system where user feedback on the chatbot's responses is collected and used for continuous improvement. This could be as simple as asking users to rate the helpfulness of responses.\n",
        "\n",
        "- Alternative Models: Consider trying different models. There are other models like OpenAI's GPT-3 or newer versions which might offer better performance, especially for complex problem-solving.\n",
        "\n",
        "- Limitations Acknowledgment: Make sure to set the right expectations and acknowledge the limitations of your chatbot. It's important for users to know that while the chatbot can assist in many ways, it might not be perfect in every scenario.\n",
        "\n",
        "- Regular Updates and Maintenance: Keep your model and libraries updated. Newer versions often come with improvements and bug fixes.\n",
        "\n",
        "- Error Handling and Logging: Implement robust error handling and logging. This will help you understand where your chatbot is failing and allow you to make targeted improvements.\n",
        "\n",
        "- Community Engagement: If possible, engage with a community of developers working on similar projects. This can provide valuable insights and shared learning opportunities."
      ],
      "metadata": {
        "id": "FhHuIOhKlMwr"
      }
    }
  ]
}