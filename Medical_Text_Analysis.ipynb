{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Medical Text Analysis\n",
        "\n",
        "\"Medical Text Analysis\" is a project that involves the use of natural language processing (NLP) and machine learning techniques to **analyze and extract** **meaningful information from medical texts, documents, or records.** This project typically focuses on healthcare and medical-related data and can have various applications in the medical field.\n",
        "\n",
        "Data Collection:\n",
        "\n",
        "The project begins with the collection of medical texts and documents, which can include electronic health records (EHRs), medical literature, clinical notes, patient surveys, medical websites, and more. These texts can be in various formats, such as text documents, PDFs, or structured databases.\n",
        "\n",
        "Text Preprocessing:\n",
        "\n",
        "The collected medical texts often require preprocessing to clean and structure the data. Common preprocessing steps include text tokenization, removing special characters, handling misspellings, and identifying and removing personally identifiable information (PII) to maintain privacy and comply with regulations like HIPAA.\n",
        "\n",
        "Information Extraction:\n",
        "\n",
        "One of the primary objectives of this project is to extract valuable information from medical texts. This includes:\n",
        "\n",
        "- Named Entity Recognition (NER): Identifying and categorizing medical entities like diseases, symptoms, medications, procedures, and anatomical terms.\n",
        "\n",
        "- Relation Extraction: Determining relationships between medical entities, such as linking symptoms to diseases or medications to patients.\n",
        "\n",
        "- Attribute Extraction: Extracting attributes associated with medical entities, such as severity, duration, dosage, and patient demographics.\n",
        "\n",
        "Medical Condition Classification:\n",
        "\n",
        "Another important task in medical text analysis is classifying medical conditions or diseases based on textual descriptions. Machine learning models can be trained to classify text into specific medical categories or conditions, aiding in diagnosis and patient management.\n",
        "\n",
        "Sentiment Analysis:\n",
        "\n",
        "Sentiment analysis can be applied to medical texts to determine the emotional tone or sentiment expressed by patients or healthcare providers. For example, identifying positive or negative sentiment in patient reviews or feedback.\n",
        "\n",
        "Topic Modeling:\n",
        "\n",
        "Medical texts can be subject to topic modeling techniques to discover common themes or topics within a corpus of documents. This can help in identifying emerging trends or research areas in healthcare.\n",
        "\n",
        "Clinical Decision Support:\n",
        "\n",
        "The insights and information extracted from medical texts can be used to support clinical decision-making. This includes assisting healthcare professionals in diagnosing diseases, recommending treatment options, or predicting patient outcomes.\n",
        "\n",
        "Compliance and Privacy:\n",
        "\n",
        "Medical text analysis must adhere to strict data security and privacy regulations, such as HIPAA in the United States. Measures need to be in place to ensure that patient data is protected and anonymized during analysis.\n",
        "Model Training and Validation:\n",
        "\n",
        "Machine learning models, including NLP models like **BERT** or clinical-specific models, may be trained and validated on medical text datasets to perform specific tasks effectively.\n",
        "\n",
        "Deployment:\n",
        "\n",
        "Once developed and validated, the medical text analysis system can be deployed in healthcare settings, research institutions, or as part of telemedicine solutions to assist healthcare providers in their work.\n",
        "\n",
        "Medical text analysis is a crucial tool for improving healthcare outcomes, supporting clinical research, and enhancing the efficiency of medical practice. It leverages NLP and AI technologies to process and extract valuable insights from the vast amount of medical information available in text form.\n",
        "\n",
        "\n",
        "Creating a comprehensive medical text analysis system in Python typically involves complex NLP models and access to medical text datasets. Below is a simplified example code that demonstrates some basic concepts of medical text analysis using Python libraries. This example focuses on extracting medical entities **(e.g., diseases, symptoms)** and classifying medical conditions from a given text.\n",
        "\n",
        "To run this code, you'll need to install the **spaCy** library and download the **en_core_med7_lg model, which is a pre-trained model for medical entity** **recognition**\n",
        "\n",
        "In this code:\n",
        "\n",
        "- We load the **en_core_med7_lg model**, which is specifically trained for medical entity recognition.\n",
        "\n",
        "- We process an example medical text that includes symptoms, a diagnosis, and prescribed medication.\n",
        "\n",
        "- We extract medical entities using the pre-trained model and classify them into categories like diseases, symptoms, and medications.\n",
        "\n",
        "- Finally, we print the extracted medical entities and classified medical conditions.\n",
        "\n",
        "Keep in mind that this is a basic illustration, and real-world medical text analysis often involves more extensive preprocessing, larger datasets, and potentially more advanced models for better accuracy and coverage."
      ],
      "metadata": {
        "id": "0_tbQWoSrC8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0.tar.gz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QZqQD09wrDOX",
        "outputId": "2ac34d55-e6b3-4b8b-c727-ad4ec1d4e3c6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0.tar.gz\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0.tar.gz (13.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting spacy<3.2.0,>=3.1.0 (from en-core-web-sm==3.1.0)\n",
            "  Downloading spacy-3.1.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.12)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.9)\n",
            "Collecting thinc<8.1.0,>=8.0.12 (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0)\n",
            "  Downloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (659 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m659.5/659.5 kB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.7.11)\n",
            "Collecting wasabi<1.1.0,>=0.8.1 (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0)\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.10)\n",
            "Collecting typer<0.5.0,>=0.3.0 (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0)\n",
            "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.31.0)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0)\n",
            "  Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2023.11.17)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.1.3)\n",
            "Building wheels for collected packages: en-core-web-sm\n",
            "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-sm: filename=en_core_web_sm-3.1.0-py3-none-any.whl size=13622635 sha256=3822f78355f368f822ed7f579a419ebaa26351e9e27195c5bd351aa571562c3c\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/6e/48/3d557772e2dba160f9e80683da52497090100cde2173ef5e34\n",
            "Successfully built en-core-web-sm\n",
            "Installing collected packages: wasabi, typer, pydantic, thinc, spacy, en-core-web-sm\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.2\n",
            "    Uninstalling wasabi-1.1.2:\n",
            "      Successfully uninstalled wasabi-1.1.2\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.0\n",
            "    Uninstalling typer-0.9.0:\n",
            "      Successfully uninstalled typer-0.9.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.13\n",
            "    Uninstalling pydantic-1.10.13:\n",
            "      Successfully uninstalled pydantic-1.10.13\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.12\n",
            "    Uninstalling thinc-8.1.12:\n",
            "      Successfully uninstalled thinc-8.1.12\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.6.1\n",
            "    Uninstalling spacy-3.6.1:\n",
            "      Successfully uninstalled spacy-3.6.1\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 3.6.0\n",
            "    Uninstalling en-core-web-sm-3.6.0:\n",
            "      Successfully uninstalled en-core-web-sm-3.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "inflect 7.0.0 requires pydantic>=1.9.1, but you have pydantic 1.8.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed en-core-web-sm-3.1.0 pydantic-1.8.2 spacy-3.1.7 thinc-8.0.17 typer-0.4.2 wasabi-0.10.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydantic",
                  "spacy",
                  "thinc",
                  "wasabi"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTRMzxbfvAw1",
        "outputId": "dbb09fdb-e3d2-4320-a70b-5ab9927c8b00"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.1.0) was trained with spaCy v3.1.0 and may not be 100% compatible with the current version (3.6.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the pre-trained medical NER model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example medical text\n",
        "medical_text = \"\"\"\n",
        "Patient presented with a severe headache and fever.\n",
        "Diagnosis: Migraine with aura.\n",
        "Prescribed medication: Sumatriptan.\n",
        "\"\"\"\n",
        "\n",
        "# Process the medical text using the NLP model\n",
        "doc = nlp(medical_text)\n",
        "\n",
        "# Extract medical entities (diseases, symptoms, medications)\n",
        "medical_entities = []\n",
        "for ent in doc.ents:\n",
        "    medical_entities.append((ent.text, ent.label_))\n",
        "\n",
        "# Print extracted medical entities\n",
        "print(\"Extracted Medical Entities:\")\n",
        "for entity, label in medical_entities:\n",
        "    print(f\"Entity: {entity}, Label: {label}\")\n",
        "\n",
        "# Classify medical conditions based on the extracted entities\n",
        "conditions = set()\n",
        "for entity, label in medical_entities:\n",
        "    if label == \"DISEASE\":\n",
        "        conditions.add(entity)\n",
        "\n",
        "# Print classified medical conditions\n",
        "print(\"\\nClassified Medical Conditions:\")\n",
        "for condition in conditions:\n",
        "    print(condition)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNsJmM2Gs_nx",
        "outputId": "f8ff23a5-aa58-41c1-ecc1-37e76141e8a3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Medical Entities:\n",
            "Entity: Sumatriptan, Label: PRODUCT\n",
            "\n",
            "Classified Medical Conditions:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more accurate and specific medical entity recognition, you can explore pre-trained medical NLP models like \"Med7\" or \"MedMentions\" that are designed specifically for medical text analysis"
      ],
      "metadata": {
        "id": "3ZCwhQ0FvuEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load a medical NER model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example medication record\n",
        "medication_record = \"\"\"\n",
        "Patient: Jane Smith\n",
        "Date: January 14, 2024\n",
        "\n",
        "Medication List:\n",
        "1. Medication: Lisinopril\n",
        "   Dosage: 10 mg\n",
        "   Frequency: Once daily\n",
        "   Indication: Hypertension\n",
        "\n",
        "2. Medication: Metformin\n",
        "   Dosage: 500 mg\n",
        "   Frequency: Twice daily\n",
        "   Indication: Diabetes mellitus\n",
        "\n",
        "3. Medication: Atorvastatin\n",
        "   Dosage: 20 mg\n",
        "   Frequency: Once daily\n",
        "   Indication: Hyperlipidemia\n",
        "\"\"\"\n",
        "\n",
        "# Process the medication record using the NLP model\n",
        "doc = nlp(medication_record)\n",
        "\n",
        "# Extract medication information (medication names, dosages, indications)\n",
        "medication_info = []\n",
        "for ent in doc.ents:\n",
        "    medication_info.append((ent.text, ent.label_))\n",
        "\n",
        "# Print extracted medication information\n",
        "print(\"Extracted Medication Information:\")\n",
        "for info, label in medication_info:\n",
        "    print(f\"Information: {info}, Label: {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0Dmz0DhvvgU",
        "outputId": "9102fb92-885f-4aa1-a4c8-3b7014b95544"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Medication Information:\n",
            "Information: Jane Smith, Label: PERSON\n",
            "Information: January 14, 2024, Label: DATE\n",
            "Information: 1, Label: CARDINAL\n",
            "Information: 10, Label: CARDINAL\n",
            "Information: Metformin, Label: PERSON\n",
            "Information: 500, Label: CARDINAL\n",
            "Information: 3, Label: CARDINAL\n",
            "Information: Atorvastatin, Label: PERSON\n",
            "Information: 20, Label: CARDINAL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load a medical NER model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example clinical note\n",
        "clinical_note = \"\"\"\n",
        "Patient: John Doe\n",
        "Date: January 14, 2024\n",
        "\n",
        "Chief Complaint: Severe abdominal pain and nausea.\n",
        "\n",
        "History of Present Illness:\n",
        "The patient presents with a two-day history of severe abdominal pain, especially in the lower right quadrant. The pain is constant and has worsened since yesterday. He also reports nausea and a low-grade fever.\n",
        "\n",
        "Physical Examination:\n",
        "On examination, the patient's abdomen is tender to palpation in the right lower quadrant. There is no rebound tenderness. Vital signs are stable.\n",
        "\n",
        "Assessment:\n",
        "Based on clinical findings, the patient is suspected to have appendicitis.\n",
        "\n",
        "Plan:\n",
        "1. Perform a complete blood count (CBC).\n",
        "2. Obtain an abdominal ultrasound.\n",
        "3. Consult a surgeon for possible appendectomy.\n",
        "\"\"\"\n",
        "\n",
        "# Process the clinical note using the NLP model\n",
        "doc = nlp(clinical_note)\n",
        "\n",
        "# Extract medical entities (symptoms, diagnosis, procedures)\n",
        "medical_entities = []\n",
        "for ent in doc.ents:\n",
        "    medical_entities.append((ent.text, ent.label_))\n",
        "\n",
        "# Print extracted medical entities\n",
        "print(\"Extracted Medical Entities:\")\n",
        "for entity, label in medical_entities:\n",
        "    print(f\"Entity: {entity}, Label: {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8lfW-plv6Td",
        "outputId": "20fd9f17-4e18-4c75-84a9-a7217c37d492"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Medical Entities:\n",
            "Entity: John Doe, Label: PERSON\n",
            "Entity: January 14, 2024, Label: DATE\n",
            "Entity: two-day, Label: DATE\n",
            "Entity: yesterday, Label: DATE\n",
            "Entity: 1, Label: CARDINAL\n",
            "Entity: 2, Label: CARDINAL\n",
            "Entity: 3, Label: CARDINAL\n"
          ]
        }
      ]
    }
  ]
}