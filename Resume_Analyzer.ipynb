{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Resume Analyzer\n",
        "\n",
        "\n",
        "The \"Resume Analyzer\" project involves building a tool that is capable of analyzing resumes and providing valuable feedback on both content and style. This tool can be incredibly useful for job seekers, career advisors, and recruiters.\n",
        "\n",
        "Here's a breakdown of the key components and functionalities of a Resume Analyzer:\n",
        "\n",
        "Resume **Parsing**: The first step of the analyzer is to parse the resume. It should be able to **extract text** and relevant information **from different** types of resume **formats, such as PDF, Word documents, or plain text files**.\n",
        "\n",
        "- Content Analysis:\n",
        "\n",
        "Keyword Extraction: The tool can extract important keywords and key phrases from the resume. These keywords may include skills, qualifications, experience, education, and more.\n",
        "\n",
        "Relevance Assessment: It assesses the relevance of the extracted content to the job position or industry. For example, it checks if the skills and experience listed match the requirements of a specific job posting.\n",
        "\n",
        "- Style Analysis:\n",
        "\n",
        "Grammar and Spelling Check: The tool performs a grammar and spelling check to identify and highlight any errors in the resume. It helps in maintaining a professional appearance.\n",
        "\n",
        "Formatting Check: It checks if the resume follows common formatting guidelines, such as consistent font usage, bullet points, indentation, and overall aesthetics.\n",
        "\n",
        "Language and Tone: The tool can also analyze the language and tone used in the resume. It ensures that the tone is appropriate for a professional document.\n",
        "\n",
        "- Feedback Generation:\n",
        "\n",
        "Content Recommendations: Based on the content analysis, the tool provides recommendations for improving the resume. It may suggest adding specific skills or qualifications.\n",
        "\n",
        "Style Recommendations: For style-related issues, it can offer suggestions on grammar corrections, formatting improvements, or language adjustments.\n",
        "\n",
        "Scoring and Summary: The tool may assign a score or rating to the resume based on its quality. Additionally, it provides a summary of the analysis, highlighting strengths and areas for improvement.\n",
        "\n",
        "Customization: Users should have the option to customize the analysis based on the job they are applying for. This can help tailor the feedback to specific positions or industries.\n",
        "\n",
        "Privacy and Data Security: Ensuring the privacy and security of the resume data is crucial. Users need to trust that their personal information is not misused.\n",
        "\n",
        "User-Friendly Interface: The tool should have a user-friendly interface that allows users to easily upload their resumes, receive feedback, and make improvements.\n",
        "\n",
        "Overall, a Resume Analyzer is a valuable tool for individuals seeking employment, as it helps them create well-crafted, error-free resumes that are more likely to make a positive impression on potential employers. Additionally, it can save time for career advisors and recruiters by providing them with more structured and relevant resume submissions.\n",
        "\n",
        "\n",
        "Building a complete Resume Analyzer is a substantial project that involves several components like document parsing, content analysis, and feedback generation. It would be impractical to provide the entire code in a single response due to its complexity. However, I can provide you with a simplified Python code example that demonstrates some aspects of resume analysis, such as parsing a resume in PDF format and performing keyword extraction.\n",
        "\n",
        "Please note that for a production-ready Resume Analyzer, you'd need to consider aspects like grammar checking, style analysis, and **integrating with a database of job descriptions**.\n",
        "\n",
        "In this code, we use the **PyPDF2 library** to extract text from a PDF resume and then perform simple keyword extraction. Note that this is a basic example, and a real Resume Analyzer would require more advanced text processing and analysis techniques.\n",
        "\n",
        "\n",
        "You can extend this code by adding features like grammar checking, style analysis, and content recommendations based on job descriptions. Additionally, consider using natural language processing libraries like spaCy or NLTK for more advanced text analysis tasks.\n"
      ],
      "metadata": {
        "id": "-BuQlmlT3sF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the PyPDF2 library\n",
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot4eOlDe8bkm",
        "outputId": "24441152-b4b2-481d-cd84-8fa78779aa16"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's how to modify the code to load your resume from Colab's file system:\n",
        "\n",
        "First, make sure your PDF resume is in the desired folder on your desktop.\n",
        "\n",
        "**In Google Colab**, you can use the following code to **upload your resume** **PDF** file:"
      ],
      "metadata": {
        "id": "lvcildwW9tc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the resume PDF file from your local machine to Colab\n",
        "uploaded = files.upload()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "9L6mvnX58AlI",
        "outputId": "225db2b8-97f4-473d-8570-f0a2cd988197"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-daac1885-6493-436d-ba8c-a8846ff0b1e3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-daac1885-6493-436d-ba8c-a8846ff0b1e3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving my resume.pdf to my resume.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " It seems that the PyPDF2 library's PdfReader expects a **file-like object** and not just the file contents as **bytes**.\n",
        "\n",
        "**To work with the bytes object**, you can use the **io.BytesIO** module to **create a file-like object from the bytes** and then use PdfReader"
      ],
      "metadata": {
        "id": "7SEHQDjL8BRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# Upload the PDF file to Colab\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Iterate through the uploaded files\n",
        "for filename, file_contents in uploaded.items():\n",
        "    # Check if the file is a PDF\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        pdf_reader = PyPDF2.PdfReader(io.BytesIO(file_contents))\n",
        "\n",
        "        # Extract and display the content page by page\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text = page.extract_text()\n",
        "            print(text)\n",
        "    else:\n",
        "        print(f\"{filename} is not a PDF file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8tO2zfnE1X5H",
        "outputId": "736a64a3-543d-469f-9be8-3b7f679dc65d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6fef60d5-8181-4461-bfc0-1c582fe62a7d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6fef60d5-8181-4461-bfc0-1c582fe62a7d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving my resume.pdf to my resume (2).pdf\n",
            "Hossein Ijadpanah  \n",
            "Student of Artifiial Intelligence and Machine Learning  \n",
            "College LaSalle , department Computer Science  \n",
            " \n",
            "   \n",
            "Phone: 5149842754  \n",
            "Email: hijadpanahsaravi@gmail.com  \n",
            "LinkedIn :  \n",
            "(2) hossein ijadpanah saravi | LinkedIn  \n",
            "GitHub:  \n",
            "hosseinijadpanah (Hossein Ijadpanah) (github.com)  \n",
            "Personal statement  \n",
            " I am seeking an internship position within the  PSP Careers  Teams , where I can \n",
            "utilize my expertise in machine learning and artificial intelligence to tackle real -world \n",
            "challenges while also gaining practical experience in the  Industries .  \n",
            " \n",
            "Education  \n",
            "LaSalle College                                                      2022-present    \n",
            " \n",
            "As a student of Artificial Intelligence and Machine Learning, I am deeply committed to \n",
            "exploring  the potential of these technologies to solve complex problems and devising novel \n",
            "solutions to real -world challenges. Through my coursework and hands -on projects, I have \n",
            "developed a solid foundation in programming with Python, data analysis, and statistica l \n",
            "modeling, and I am enthusiastic about further expanding my knowledge and leveraging my \n",
            "skills to drive the ongoing evolution of these dynamic fields.  \n",
            " \n",
            "ETS university                                                            2018 -2021  \n",
            "As a master student of Civil Engineering with a focus on sustainable solutions, I am \n",
            "committed to developing innovative approaches to environmental challenges. Specifically, \n",
            "my research has focused on optimizing water recycling, through the use of data -driven \n",
            "approaches. where efficient and effective water management is critical to both economic and \n",
            "ecological sustainability. Through my coursework and practical experience, I have developed \n",
            "a deep understanding of the engineering principles involved in water  treatment and \n",
            "management, as well as the potential of machine learning to drive sustainability in water \n",
            "management. I am eager to apply this knowledge to real -world problems and work \n",
            "collaboratively with organizations to develop efficient and effective wa ter management \n",
            "strategies.  The projects that I involved in this period was:  \n",
            "- Water Quality Analysis: Develop an AI algorithm that can analyze water quality data and \n",
            "identify any contaminants or anomalies.  \n",
            "- Water Management and Optimization:  Develop an AI model designed to optimize water \n",
            "management practices by predicting water demand and quality in specific regions, while \n",
            "recommending the most effective conservation and distribution strategies.  \n",
            " \n",
            "Concordia University                                             2016 -2018  \n",
            "As a research assistant, my focus has been on the development of analytical data features and \n",
            "the application of machine learning techniques to optimize neural network models. Through \n",
            "my experience in data processing, statistical analysis, and algorithm de velopment, I have \n",
            "gained a deep understanding of the tools and techniques required to analyze large amounts \n",
            "of data, identify trends and patterns, and draw valuable insights. Addit ionally, I have \n",
            "expertise in the installation, integration, and operation of computer -based systems, allowing  \n",
            " \n",
            "Summary  \n",
            "Data scientist  \n",
            "Research assistant AI/ML  \n",
            "M.Sc. civil engineering  \n",
            "M.Sc. mineral processing engineering  \n",
            "B.Sc. mining engineering  \n",
            " \n",
            " \n",
            "Projects  \n",
            "Artificial Intelligence  \n",
            "Machine Learning  \n",
            "Machine Learning  \n",
            "Operation Research  \n",
            "Big Data  \n",
            "   \n",
            " Skills  \n",
            "Comfortable working with Python  \n",
            "MATLAB  \n",
            "SQL  and NoSQL  \n",
            " TensorFlow  \n",
            "Keras  \n",
            "PyTorch  \n",
            "Other machine learning frameworks  \n",
            "Problem Solving   \n",
            "Data Cleaning  \n",
            "Data Analysis  \n",
            "Neural Networks  \n",
            "Feature Engineering  \n",
            "Mathematical Modelling  \n",
            "Statistics   \n",
            "Algorithms  \n",
            " SQL / NoSQL/  MongoDB  \n",
            "RNN/CNN/ LSTM  \n",
            "NLP  \n",
            "Computer Vision  \n",
            "Data Visualization  \n",
            "Operation Research  \n",
            "Google -Colab  \n",
            "Jupyter -Notebook  \n",
            "Pandas_datareader  \n",
            "Git \n",
            "Excellent communication interpersonal  \n",
            "Skills \n",
            " \n",
            "me to effectively manage complex projects from start to finish.  During  this time I involved \n",
            "to these project:   \n",
            " \n",
            "- Waste Management Optimization : Create an AI model that can optimize the waste \n",
            "management process by predicting the amount of waste generated in a particular area and \n",
            "suggesting the most efficient disposal methods.  \n",
            " \n",
            "- Renewable Energy Forecasting: Use AI to predict the amount of renewable energy that can \n",
            "be generated in a specific location based on weather patterns, solar radiation, and other \n",
            "factors.  \n",
            " \n",
            " \n",
            "Pervious E ducation  \n",
            " \n",
            "University of Tarbiat modares, Tehran, Iran — M.Sc. mineral processing and environmental \n",
            "engineering  \n",
            "2008 - 2011  \n",
            "GPA: 17.39/20  \n",
            "Dissertation: synthesis of nano TiO2 particles and its application for industrial wastewater treatment  \n",
            "Supervisor: Dr. Khodadadi -darban  \n",
            " \n",
            "Isfahan University of Technology,  Isfahan, Iran — B.Sc. mining engineering  \n",
            "2004 - 2008  \n",
            "Dissertation: Slope stability analysis in a Granite Quarry (Case study: hozmahi mine, Iran)  \n",
            "GPA: 16.8/20  \n",
            " \n",
            "Sarcheshmeh Copper Complex, Iran.                              2012 –2015  \n",
            "➢ Operate and respond to all aspects of control room operation including emergency response.  \n",
            "➢ Monitor machine cycle and mill operation to detect jamming and to ensure that products \n",
            "conform to specification.  \n",
            "➢ Completed and entered quality records, reports  and machine logs.  \n",
            "➢ Examine, inspect  and measure raw materials, feeds, effluents and products.  \n",
            "➢ Implemented safety precaution such as donning personal protective equipment, using safety \n",
            "lines, using Lock out / Tag out procedures, etc.  \n",
            "➢  Practiced proper safety and sanitary regulations  \n",
            " \n",
            "  \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Python Libraries  \n",
            "Tensorflow, Keras, PyTorch,  \n",
            "Scikit -Learn, Pandas,  \n",
            "NumPy Matplotlib.  \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "LANGUAGES  \n",
            "English  \n",
            "French \n",
            "Projects  \n",
            " \n",
            "Car License Plate Detection  \n",
            "In the context of the Car License Plate Detection, this project is focused on object detection and \n",
            "localization, particularly for identifying and recognizing car license plates within images. The dataset \n",
            "consists of 433 images, each containing one or more car license plates, with bounding box annotations \n",
            "provided in the widely used PASCAL VOC format. The project leverages deep learning capabilities \n",
            "through TensorFlow and Keras. Data preprocessing techniques include image resizing, normalization, \n",
            "and data lo ading using ImageDataGenerator. A VGG16 -based convolutional neural network (CNN) \n",
            "model is constructed, with both the base model and custom head for regression tasks. The model is \n",
            "trained with mean squared error (MSE) loss and Adam optimizer.   \n",
            "ComputerVision/Computer_Vision_3_Vehicle_Plate_Detection.ipynb at main · hosseinijadpanah/ComputerVision (github.com)  \n",
            " \n",
            "The Face Recognition   \n",
            "The Face Recognition project is centered around deep learning techniques for face recognition, \n",
            "primarily utilizing triplet loss and contrastive loss during training. It addresses challenges related to \n",
            "limited data for each class within the dataset, employi ng Python libraries such as numpy, torchvision, \n",
            "matplotlib, Adam optimizer, and more alongside data preprocessing techniques, including resizing \n",
            "and augmentation. The data is organized using a custom TripletDataset class, generating triplets of \n",
            "anchor, pos itive, and negative images. Triplet loss is used to guide the network to minimize the \n",
            "distance between anchor and positive samples while maximizing it between anchor and negative \n",
            "samples. Data augmentation is applied to improve model generalization, and a pre-trained CNN model \n",
            "like ResNet -50 is employed for feature extraction. The training loop computes triplet loss, updates the \n",
            "model through backpropagation, and monitors training and validation losses. Additionally, there is an \n",
            "option for a custom training  loop. Testing and visualization of training, validation, and test results are \n",
            "included, offering flexibility in model architecture selection and model summary visualization.  \n",
            "FaceDetection -TripletLoss/Face recognition.ipynb at main · hosseinijadpanah/FaceDetection -TripletLoss (github.com)  \n",
            " \n",
            "The Casting Quality Inspection  \n",
            "This project aims to automate quality inspection for casting products, focusing on detecting various \n",
            "defects like blow holes, pinholes, burrs, and shrinkage defects. In the casting industry, defects are \n",
            "undesirable, necessitating their detection and removal. Th e dataset is categorized into \"Defective\" and \n",
            "\"Non -defective\" items. Python libraries like numpy, matplotlib, sklearn, confusion_matrix, optim, \n",
            "torchvision, TensorFlow, and Keras are employed alongside data preprocessing techniques, including \n",
            "resizing and augmentation. The model utilizes Convolutional Neural Networks (CNNs) with binary \n",
            "cross -entropy loss and the Adam optimizer. Early stopping is implemented based on accuracy, and \n",
            "training progress is visualized. Ultimately, this project enhances efficiency and accuracy compared to \n",
            "manual inspection methods.  \n",
            "ComputerVision/Copy_of_Computer_Vision_2_casting_quality_inspection (1).ipynb at main · hosseinijadpanah/ComputerVision (gith ub.com)  \n",
            " \n",
            "Text Classification with FastAPI and Streamlit  \n",
            "This project involves text classification using the '20 Newsgroups' dataset, comprising around 20,000 \n",
            "newsgroup documents distributed across 20 categories. a Logistic Regression model is trained for text \n",
            "classification. Initially, the dataset is loaded, pr eprocessed, and split into training and testing sets. Text \n",
            "features are extracted using TF -IDF vectorization. The Logistic Regression model achieves an \n",
            "accuracy of approximately 73%, with detailed classification metrics for each category. then an API is \n",
            "created using FastAPI, enabling text classification predictions, and finally demonstrates the integration \n",
            "of the model into a Streamlit app, allowing users to input text and receive class predictions. The \n",
            "combination of FastAPI and Streamlit provides a user -friendly interface for real -time text \n",
            "classification with the pre -trained model.  \n",
            "PipeLine/end_to_end_machine_learning_content_based_recommendation_system_with_NLP_text_classification.ipynb at \n",
            "main · hosseinijadpanah/PipeLine (github.com)  \n",
            " \n",
            "Content -Based Movie Recommendation System  \n",
            "Content -based recommendation systems are designed to provide personalized movie \n",
            "recommendations to users based on the characteristics of movies and user preferences. This project \n",
            "focuses on constructing a content -based movie recommendation system using Pyt hon. The libraries   \n",
            " \n",
            "Honors and Awards  \n",
            " \n",
            " \n",
            " Ranked 10 st among more  \n",
            "than 200 selected superior  \n",
            "invention  in international  \n",
            "festival of nanotechnology  \n",
            "prototype, in Iran, in 2013.  \n",
            " \n",
            " \n",
            "Ranked 36st among more  \n",
            "than 14,000 participants in  \n",
            "Iranian undergraduate  \n",
            "nationwide entrance exam, 2008.  \n",
            " \n",
            " \n",
            " \n",
            "Rank 5300st in the nationwide  \n",
            "University Entrance Exam among  \n",
            "500,000 participants in Iran, 2003.  \n",
            " \n",
            " \n",
            "Iran patent: 123654324576  \n",
            "“ waste water treatment package  \n",
            "by using titana nanoparticle  \n",
            " that coated on  activated carbon”  \n",
            "2013  \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "CERTIFICATES  \n",
            " \n",
            "Coursera (deeplearning.ai)  \n",
            "Deep Learning Specialization.  \n",
            "Master Deep Learning, and  \n",
            "Break into AI.  \n",
            "1- Convolutional Neural  \n",
            "Networks by deeplearning.ai  \n",
            "on Coursera.  \n",
            "2- Neural Networks and Deep  \n",
            "and techniques used in this project include pandas, numpy, scikit -learn, NLTK, and then make \n",
            "Streamlit for creating an interactive recommendation application.  The project begins by preprocessing \n",
            "a dataset of movie information, cleaning text data, and merging relevant columns to create a \n",
            "comprehensive document for each movie. Text cleaning involves lowercasing, removing special \n",
            "characters, tokenization, stopwords  removal, stemming, and lemmatization. Then, TF -IDF \n",
            "vectorization is applied to transform the tex t data into numerical features. Cosine similarity is used to \n",
            "compute similarity scores between movies. The top -K similar movies for each movie in the dataset \n",
            "are stored in a DataFrame. This recommendation system can assist users in discovering movies simil ar \n",
            "to their preferences, improving the movie -watching experience.  \n",
            "NLP/Content_based_recommendation_systems.ipynb at main · hosseinijadpanah/NLP (github.com)  \n",
            " \n",
            "Sentiment Analysis with Hugging Face Transformers: Multilingual BERT Model  \n",
            "This project focuses on sentiment analysis using Hugging Face Transformers, employing a \n",
            "multilingual BERT model. After installing the required python libraries and upgrading Transformers, \n",
            "the project showcases two approaches to sentiment analysis. The firs t uses the default sentiment \n",
            "analysis pipeline, where a pre -trained model (distilbert -base-uncased -finetuned -sst-2-english) is used \n",
            "to analyze text sentiment, providing positive or negative labels with corresponding scores. The second \n",
            "approach demonstrates  the customization of the model to a multilingual BERT variant, allowing \n",
            "sentiment analysis for text in various languages. This model yields more nuanced sentiment labels \n",
            "such as \"5 stars\" and \"1 star,\" along with associated confidence scores. These exampl es provide insight \n",
            "into the sentiment of text snippets, making it a valuable tool for analyzing sentiment across languages \n",
            "and fine -tuning models for specific applications.  \n",
            "PipeLine/SentimentAnalysisHuggingFace_pipeline.ipynb at main · hosseinijadpanah/PipeLine (github.com)  \n",
            " \n",
            "Comprehensive NLP Text Analysis Toolkit: Pipelines for Text Classification, Name Identity, and \n",
            "Sentiment Analysis  \n",
            "This project showcases the power of Natural Language Processing (NLP) by implementing three \n",
            "essential processing pipelines: text classification, named entity recognition , and sentiment analysis. \n",
            "Python serves as the primary programming language, with key NLP libraries such as spaCy and \n",
            "TextBlob playing pivotal roles. The text classification pipeline allows users to categorize input text \n",
            "into customized predefined categories, while the named entity pipeline identifies named entities like \n",
            "names, locations, a nd organizations. The sentiment analysis pipeline provides sentiment labels for \n",
            "input text. This project offers a user -friendly web interface for text analysis, where users input text \n",
            "and receive instant results for text classification, named entity, and s entiment analysis. Moreover, the \n",
            "project supports model saving and loading using pickle files and integrates seamlessly with Google \n",
            "Drive for model storage, ensuring user -friendly and efficient NLP analysis.  \n",
            "PipeLine/Multiple_NLP_Pipelines_to_perform_different_tasks_on_the_same_input_text.ipynb at main · hosseinijadpanah/PipeLine \n",
            "(github.com)  \n",
            " \n",
            "Spam Message Classification Using LSTM  \n",
            "This project presents a thorough exploration of text message classification, aiming to distinguish \n",
            "between spam and non -spam (ham) messages through machine learning and natural language \n",
            "processing techniques. It begins with data acquisition from Kaggle, fo llowed by data inspection and \n",
            "exploratory analysis in a Pandas DataFrame. Key data preprocessing steps include label encoding, \n",
            "dataset splitting, and text preprocessing. The core of the project lies in building a sequential deep \n",
            "learning model using Keras,  comprising embedding and LSTM layers for word representation and \n",
            "sequence modeling, culminating in a dense layer for binary classification. The model is meticulously \n",
            "trained and evaluated, yielding an impressive 98.57% accuracy in identifying spam message s. In \n",
            "summary, this project illustrates the end -to-end process of text classification and showcases the \n",
            "efficacy of deep learning in spam detection from text data.  \n",
            "NLP/SpamDetectionWithLSTM.ipynb at main · hosseinijadpanah/NLP (github.com)  \n",
            " \n",
            "Text Data Preprocessing and Feature Extraction using Python and NLTK  \n",
            "This project involved using Python and NLTK( Natural Language Toolkit) for text data preprocessing \n",
            "and feature extraction. The text data was then preprocessed using functions such as tokenization, \n",
            "stemming, and lemmatization then features were extracted from the preprocessed text using functions \n",
            "such as bag -of-words, n -grams, and TF -IDF. The resulting features were stored in a dictionary format Learning by deeplearning.ai on  \n",
            "Coursera.  \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "DataCamp  \n",
            " \n",
            "-Intermediate Python for Data  Science  \n",
            "-Supervised Learning with  Scikit -Learn  \n",
            "-Manipulating Data Frames  with pandas  \n",
            "-Introduction to Data  Visualization in Python  \n",
            "- Introduction to Machine  Learning  \n",
            "- Deep Learning in Python  \n",
            " \n",
            "for further analysis or use. Overall, this project demonstrated the power of Python and NLTK for text \n",
            "data preprocessing and feature extraction.  \n",
            "NLP/WordEmbedding.ipynb at main · hosseinijadpanah/NLP (github.com)  \n",
            "Garbage Classification with FastAPI, Streamlit, and PyTorch  \n",
            "This project demonstrates an end -to-end solution for garbage classification, from training a deep \n",
            "learning model to creating a user interface for classifying garbage images efficiently. the garbage \n",
            "classification dataset, which contains six classes: cardbo ard, glass, metal, paper, plastic, and trash. \n",
            "Train a deep learning model (ResNet50) on the dataset, achieving a test accuracy of approximately \n",
            "73%. It enables users to upload images via Streamlit, receive real -time classification results, and \n",
            "visualize th e entire process seamlessly.  \n",
            "PipeLine/end_to_end_machine_learning_pipeline_for_image_classification_in_garbage_classification.ipynb at main · \n",
            "hosseinijadpanah/PipeLine (github.com)  \n",
            " \n",
            "Predicting Breast Cancer Diagnosis  \n",
            "A Machine Learning Approach involves the application of various machine learning algorithms and \n",
            "techniques to create a robust model for breast cancer diagnosis. This comprehensive approach \n",
            "encompasses decision trees, random forests, logistic regression, an d neural networks, leveraging their \n",
            "unique strengths in classification tasks. Furthermore, hyperparameter tuning is conducted using \n",
            "GridSearchCV, RandomizedSearchCV, and Optuna to optimize the model's performance. By utilizing \n",
            "a combination of these algori thms and hyperparameter optimization methods, this project aims to \n",
            "provide an accurate and reliable tool for diagnosing breast cancer, potentially contributing to early \n",
            "detection and improved patient outcomes in the field of healthcare.  \n",
            "hosseinijadpanah/Predicting -Breast -Cancer -Diagnosis -A-Machine -Learning -Approach: develop and evaluate machine learning models for \n",
            "breast cancer diagnosis. By utilizing the dataset's features, the analysis aims to predict whether a given biopsy is malignan t (indicating the \n",
            "presence of cancer) or benign (non -cancerous). (github.com)  \n",
            "recognizing handwritten digits with high accuracy(  more than 98 %) \n",
            "this project proceeds by generating training and testing datasets, followed by the establishment of data \n",
            "loaders to efficiently load data in batches. The core of the project entails the training of a Convolutional \n",
            "Neural Network (CNN) using the MNIST datas et. To optimize the learning process, the script \n",
            "configures a Stochastic Gradient Descent (SGD) optimizer and defines a Cross Entropy Loss function. \n",
            "I developed a deep neural network for image classification using TensorFlow. The development \n",
            "process includ ed gathering and preprocessing a dataset of labeled images, I use MNIST and ImageNet  \n",
            "datasets, resizing the images, normalizing pixel values, and splitting the data into training and \n",
            "validation sets, training the model using the specified loss function an d optimizer, evaluating the \n",
            "performance of the model on the validation dataset, and testing the accuracy of the model on a separate \n",
            "dataset. By following this process, I was able to fine -tune the hyperparameters of the neural network \n",
            "and optimize the model  for improved performance.  \n",
            "ObjectDetection/number_detection.ipynb at main · hosseinijadpanah/ObjectDetection (github.com)  \n",
            " \n",
            "Publications  \n",
            " \n",
            "➢ Intelligent Tools to Model Photocatalytic Degradation of Beta -Naphtol by Titanium  \n",
            "Dioxide Nanoparticles \" Journal of Chemometrics  \n",
            "(http://onlinelibrary.wiley.com/doi/10.1002/cem.2907/epdf?r3_referer=wol&tracking  \n",
            "_action=preview_click&show_checkout=1&purchase_referrer=onlinelibrary.wiley.co  \n",
            "m&purchase_site_license=LICENSE_DENIED ) ,  \n",
            "➢ Photocatalytic decomposition of cyanide in pure water by biphasic  \n",
            "titanium dioxide nanoparticles \" Journal of Desalination and Water  \n",
            "Treatment( http://www.tandfonline.com/doi/abs/10.1080/19443994.2015.1108239?jou  \n",
            "rnalCode=tdwt20 ) ,  \n",
            "➢ Synthesis, characterization, and photocatalytic activity of TiO2 -SiO2 nanocomposites  \n",
            "(http://www.tandfonline.com/doi/abs/10.1080/19443994.2015.1076738?journalCode=  \n",
            "tdwt20 )  \n",
            "➢ Modified Diatomite -Supported CuO -TiO2 Composite: Preparation, Characterization  \n",
            "and Catalytic CO Oxidation  (http://www.sciencedirect.com/science/article/pii/S1876107015  \n",
            "002370 ), \n",
            "➢ Synthesis of  \n",
            "Titanium Dioxide Nanoparticles for Photocatalytic Degradation of Cyanide in  \n",
            "Wastewater ( http://www.tandfonline.com/doi/abs/10.1080/00032719.2014.880170#previe  \n",
            "w)  \n",
            " \n",
            " \n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import re\n",
        "import io  # Import the io module for working with bytes\n",
        "\n",
        "# Function to extract text from a PDF resume\n",
        "def extract_text_from_pdf(file_contents):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        pdf_reader = PyPDF2.PdfReader(io.BytesIO(file_contents))\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF: {str(e)}\")\n",
        "    return text\n",
        "\n",
        "# Function to perform keyword extraction from the resume\n",
        "def extract_keywords(text):\n",
        "    keywords = []\n",
        "    # Define some sample keywords (you can extend this list)\n",
        "    sample_keywords = [\"Python\", \"Machine Learning\", \"Data Analysis\", \"SQL\", \"Teamwork\"]\n",
        "    for keyword in sample_keywords:\n",
        "        # Using regular expression to find keyword occurrences\n",
        "        occurrences = len(re.findall(rf'\\b{keyword}\\b', text, flags=re.IGNORECASE))\n",
        "        keywords.append((keyword, occurrences))\n",
        "    return keywords\n",
        "\n",
        "# Assuming you have already uploaded the file in the previous code cell\n",
        "uploaded_file_contents = next(iter(uploaded.values()))\n",
        "\n",
        "# Extract text from the uploaded PDF resume\n",
        "resume_text = extract_text_from_pdf(uploaded_file_contents)\n",
        "\n",
        "# Extract keywords from the resume\n",
        "resume_keywords = extract_keywords(resume_text)\n",
        "\n",
        "# Print the extracted keywords and their occurrences\n",
        "for keyword, occurrences in resume_keywords:\n",
        "    print(f\"Keyword: {keyword}, Occurrences: {occurrences}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZ0OhQlW3sVZ",
        "outputId": "69c130ec-9fc3-44e3-e405-3be31c669ec3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyword: Python, Occurrences: 13\n",
            "Keyword: Machine Learning, Occurrences: 12\n",
            "Keyword: Data Analysis, Occurrences: 2\n",
            "Keyword: SQL, Occurrences: 2\n",
            "Keyword: Teamwork, Occurrences: 0\n"
          ]
        }
      ]
    }
  ]
}