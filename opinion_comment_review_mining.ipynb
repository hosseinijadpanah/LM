{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Opinion Mining\n",
        "\n",
        "Sentiment analysis, often referred to as opinion mining, is a common task in natural language processing (NLP) that involves classifying sentiments expressed in text, typically into categories like positive, negative, and neutral. For this example, we'll focus on review analysis using a dataset of Amazon or Netflix reviews.\n",
        "\n",
        "We'll use a popular dataset for sentiment analysis: the IMDb movie reviews dataset, available through TensorFlow datasets. This dataset is similar in structure to Amazon and Netflix reviews and serves as a good example for sentiment analysis.\n",
        "\n",
        "\n",
        "However, due to data availability constraints in this environment, I'll guide you through a hypothetical example using a similar approach, which you can adapt to Amazon or Netflix reviews.\n",
        "\n",
        "\n",
        "**Adaptation for Amazon or Netflix Reviews**\n",
        "\n",
        "To adapt this code for Amazon or Netflix reviews:\n",
        "\n",
        "- Data Acquisition: Obtain a dataset of Amazon or Netflix reviews. These datasets can often be found on websites like Kaggle or through APIs.\n",
        "- Preprocessing: You may need to preprocess the reviews (tokenization, padding) similarly to the IMDb dataset. Be mindful of the dataset's structure and content.\n",
        "- Model Tuning: Depending on the dataset's complexity and nuances, you might need to adjust the model architecture, hyperparameters, or even consider more advanced models like Transformers.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sZRsmXhuMBLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup and Data Loading\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "# Set parameters for the dataset and model\n",
        "max_features = 5000  # number of words to consider as features\n",
        "maxlen = 100  # cut reviews after this number of words\n",
        "embedding_size = 64\n",
        "\n",
        "# Load the IMDb dataset\n",
        "print(\"Loading data...\")\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Pad sequences for consistency\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v42K8OKMMBb5",
        "outputId": "9ad292d2-81bc-446e-d44b-580f6496bda0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "x_train shape: (25000, 100)\n",
            "x_test shape: (25000, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Building\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "FYsIPUo_M-HR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the Model\n",
        "# Train the model\n",
        "batch_size = 32\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=batch_size, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fzs79ilbM-J9",
        "outputId": "1203831f-b3c2-4642-80a4-6952b3c12cd3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "625/625 [==============================] - 127s 199ms/step - loss: 0.4418 - accuracy: 0.7857 - val_loss: 0.3638 - val_accuracy: 0.8390\n",
            "Epoch 2/5\n",
            "625/625 [==============================] - 123s 197ms/step - loss: 0.3094 - accuracy: 0.8705 - val_loss: 0.3585 - val_accuracy: 0.8464\n",
            "Epoch 3/5\n",
            "625/625 [==============================] - 128s 204ms/step - loss: 0.2621 - accuracy: 0.8941 - val_loss: 0.3600 - val_accuracy: 0.8426\n",
            "Epoch 4/5\n",
            "625/625 [==============================] - 123s 198ms/step - loss: 0.2256 - accuracy: 0.9096 - val_loss: 0.3899 - val_accuracy: 0.8428\n",
            "Epoch 5/5\n",
            "625/625 [==============================] - 123s 197ms/step - loss: 0.2036 - accuracy: 0.9208 - val_loss: 0.4229 - val_accuracy: 0.8418\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f2795d3b850>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "# Evaluate the model performance\n",
        "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_v3CwbEM-Np",
        "outputId": "ad588e74-d583-4140-89eb-6e72113f8f0a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 28s 35ms/step - loss: 0.4344 - accuracy: 0.8359\n",
            "Test score: 0.4344063699245453\n",
            "Test accuracy: 0.8358799815177917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same approach can be extended and customized for specific datasets like Amazon or Netflix reviews."
      ],
      "metadata": {
        "id": "GmjJ4IUMNsJn"
      }
    }
  ]
}